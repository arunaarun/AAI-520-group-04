{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T18:40:45.977147Z","iopub.status.busy":"2024-10-18T18:40:45.976176Z","iopub.status.idle":"2024-10-18T18:40:45.985144Z","shell.execute_reply":"2024-10-18T18:40:45.984161Z","shell.execute_reply.started":"2024-10-18T18:40:45.977103Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Contents of /kaggle/input:\n","/kaggle/input/cornell-moviedialog-corpus/movie_conversations.txt\n","/kaggle/input/cornell-moviedialog-corpus/README.txt\n","/kaggle/input/cornell-moviedialog-corpus/chameleons.pdf\n","/kaggle/input/cornell-moviedialog-corpus/movie_titles_metadata.txt\n","/kaggle/input/cornell-moviedialog-corpus/movie_characters_metadata.txt\n","/kaggle/input/cornell-moviedialog-corpus/movie_lines.txt\n","/kaggle/input/cornell-moviedialog-corpus/.DS_Store\n","/kaggle/input/cornell-moviedialog-corpus/raw_script_urls.txt\n"]}],"source":["import os\n","\n","print(\"Contents of /kaggle/input:\")\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{},"source":["### 50K Samples Data Load from Cornell"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T19:34:59.482803Z","iopub.status.busy":"2024-10-18T19:34:59.481901Z","iopub.status.idle":"2024-10-18T19:35:02.196127Z","shell.execute_reply":"2024-10-18T19:35:02.195064Z","shell.execute_reply.started":"2024-10-18T19:34:59.482757Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 50000 conversation pairs\n","Example pair:\n","Input: Movie: Unknown Movie\n","Line: You can't go in there. They know you're with Ruiz.\n","Target: You got that right.\n"]}],"source":["import re\n","from pathlib import Path\n","import random\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_linear_schedule_with_warmup, DataCollatorForLanguageModeling\n","from torch.utils.data import DataLoader\n","from datasets import Dataset\n","from tqdm import tqdm\n","import time\n","import numpy as np\n","\n","def load_cornell_data(num_samples=50000):\n","    base_path = Path(\"/kaggle/input/cornell-moviedialog-corpus\")\n","    \n","    # Load movie lines\n","    lines = {}\n","    with open(base_path / 'movie_lines.txt', 'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            parts = line.strip().split(' +++$+++ ')\n","            if len(parts) == 5:\n","                lines[parts[0]] = parts[4]\n","    \n","    # Load conversations\n","    conversations = []\n","    with open(base_path / 'movie_conversations.txt', 'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            parts = line.strip().split(' +++$+++ ')\n","            if len(parts) == 4:\n","                conv = eval(parts[3])\n","                conversations.append(conv)\n","    \n","    # Load movie metadata\n","    movie_metadata = {}\n","    with open(base_path / 'movie_titles_metadata.txt', 'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            parts = line.strip().split(' +++$+++ ')\n","            if len(parts) == 6:\n","                movie_metadata[parts[0]] = parts[1]  # Movie name\n","    \n","    # Create input-output pairs with context\n","    pairs = []\n","    for conversation in conversations:\n","        for i in range(len(conversation) - 1):\n","            if conversation[i] in lines and conversation[i+1] in lines:\n","                input_text = lines[conversation[i]]\n","                target_text = lines[conversation[i+1]]\n","                movie_id = conversation[i].split('_')[0]  # Extract movie ID from the line ID\n","                movie_name = movie_metadata.get(movie_id, \"Unknown Movie\")\n","                context = f\"Movie: {movie_name}\\nLine: \"\n","                pairs.append((context + input_text, target_text))\n","    \n","    # Shuffle and select subset\n","    random.shuffle(pairs)\n","    pairs = pairs[:num_samples]\n","    \n","    input_texts, target_texts = zip(*pairs)\n","    \n","    return list(input_texts), list(target_texts)\n","\n","# Load data\n","inputs, targets = load_cornell_data(num_samples=50000)\n","print(f\"Loaded {len(inputs)} conversation pairs\")\n","print(\"Example pair:\")\n","print(f\"Input: {inputs[0]}\")\n","print(f\"Target: {targets[0]}\")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T19:36:38.610470Z","iopub.status.busy":"2024-10-18T19:36:38.610072Z","iopub.status.idle":"2024-10-18T19:36:41.274828Z","shell.execute_reply":"2024-10-18T19:36:41.273721Z","shell.execute_reply.started":"2024-10-18T19:36:38.610431Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","GPU Available: True\n","GPU Device Name: Tesla P100-PCIE-16GB\n"]}],"source":["# Set up model and tokenizer\n","model_name = \"microsoft/DialoGPT-medium\"  # Consider trying \"facebook/blenderbot-400M-distill\" as an alternative\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# Move model to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","print(f\"Using device: {device}\")\n","print(f\"GPU Available: {torch.cuda.is_available()}\")\n","print(f\"GPU Device Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Dialogpt Medium"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T19:37:08.533631Z","iopub.status.busy":"2024-10-18T19:37:08.532994Z","iopub.status.idle":"2024-10-18T22:10:59.472470Z","shell.execute_reply":"2024-10-18T22:10:59.471475Z","shell.execute_reply.started":"2024-10-18T19:37:08.533582Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e202db1efee4a838db932da9ecaae0b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1: 100%|██████████| 6250/6250 [30:43<00:00,  3.39it/s, loss=3.0547]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5 completed in 1843.16 seconds. Average Loss: 2.9671\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 6250/6250 [30:40<00:00,  3.40it/s, loss=3.2201]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/5 completed in 1840.63 seconds. Average Loss: 2.7111\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 6250/6250 [30:45<00:00,  3.39it/s, loss=2.3707]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/5 completed in 1845.73 seconds. Average Loss: 2.5907\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 6250/6250 [30:48<00:00,  3.38it/s, loss=2.2364]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/5 completed in 1848.17 seconds. Average Loss: 2.5063\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 6250/6250 [30:46<00:00,  3.38it/s, loss=2.5911]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/5 completed in 1846.56 seconds. Average Loss: 2.4535\n","Training completed in 9224.25 seconds (153.74 minutes)\n"]},{"data":{"text/plain":["('./fine_tuned_dialogpt_medium/tokenizer_config.json',\n"," './fine_tuned_dialogpt_medium/special_tokens_map.json',\n"," './fine_tuned_dialogpt_medium/vocab.json',\n"," './fine_tuned_dialogpt_medium/merges.txt',\n"," './fine_tuned_dialogpt_medium/added_tokens.json',\n"," './fine_tuned_dialogpt_medium/tokenizer.json')"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Prepare dataset\n","dataset = Dataset.from_dict({\"input\": inputs, \"target\": targets})\n","\n","def tokenize_function(examples):\n","    inputs = [inp + tokenizer.eos_token + tgt + tokenizer.eos_token for inp, tgt in zip(examples[\"input\"], examples[\"target\"])]\n","    return tokenizer(inputs, truncation=True, padding=False, max_length=256)\n","\n","tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n","\n","# Use dynamic padding\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, \n","    mlm=False,\n","    pad_to_multiple_of=8  # Optimize for tensor cores\n",")\n","\n","train_dataloader = DataLoader(\n","    tokenized_dataset, \n","    shuffle=True, \n","    batch_size=8, \n","    collate_fn=data_collator\n",")\n","\n","# Setup optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 5\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n","\n","# Training loop\n","model.train()\n","total_start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    total_loss = 0\n","    \n","    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}\")\n","    \n","    for i, batch in progress_bar:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","        \n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        \n","        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n","    \n","    avg_loss = total_loss / len(train_dataloader)\n","    epoch_time = time.time() - epoch_start_time\n","    \n","    print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_time:.2f} seconds. Average Loss: {avg_loss:.4f}\")\n","\n","total_time = time.time() - total_start_time\n","print(f\"Training completed in {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n","\n","# Save the model\n","model.save_pretrained(\"./fine_tuned_dialogpt_medium\")\n","tokenizer.save_pretrained(\"./fine_tuned_dialogpt_medium\")\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T23:10:55.061398Z","iopub.status.busy":"2024-10-18T23:10:55.060638Z","iopub.status.idle":"2024-10-18T23:11:10.352850Z","shell.execute_reply":"2024-10-18T23:11:10.351957Z","shell.execute_reply.started":"2024-10-18T23:10:55.061361Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt: Question: Why does Cameron suggest that Bianca needs to learn how to lie?\n","Generated response: Because she was telling the truth when she told you she was pregnant. She told you it was because she was tired. And that's what happened. But you've got to think that there's something else going on here. I mean, she's got to be lying. She's got that look in her eyes that's just weird. And then she's like, oh yeah... she's lying. And I\n","\n","Prompt: Statement: Describe how Bianca feels about becoming a persona she can't quit.\n","Generated response: I want to be a big show like Bianca. I want to make some big plays. I have to show off and do what I can. I'm just not good enough to be Bianca without being a big play. I've tried that in every sport I've played, in every competition. But I can't do it in this one. I can barely do it as myself. I don't\n","\n","Prompt: Question: What specific comment did Guillermo make about Bianca's hair color?\n","Generated response: He said it was too light brown for her. That was the only time he ever said anything about her hair color. He also said she was too pale. That she looked like a white woman in a black dress. That her eyes were too dark. That everything about her was too black for her and she looked too young. He never said she wasn't beautiful or wasn't a good person. He just\n","\n","Prompt: Statement: Explain Kat's bad experience with a guy who broke up with her.\n","Generated response: You didn't have to explain it to me. I just read about it. It's a good story. I had a good time. I hope you had a great time. You're a good writer. I'm glad you enjoyed it. You should write more like you enjoy writing. I loved it. I was worried you wouldn't like it but you really did. I mean you're so good at it\n","\n","Prompt: Question: How does Bianca react to Joey's aspirations in his modeling career?\n","Generated response: Bianca's a very supportive woman. She's always been supportive of me. I'm glad to have her as a friend. I think it's going to be a great thing for us both. We're going to make good friends. I hope you're right. Maybe it's time to start looking for a new home for your work. Maybe you and your brother can rent it out to a good agent\n","\n","Perplexity for prompt: 7.55\n","Perplexity for prompt: 10.11\n","Perplexity for prompt: 6.92\n","Perplexity for prompt: 8.00\n","Perplexity for prompt: 8.16\n","\n","Average Perplexity: 8.15\n"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Load the fine-tuned model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_dialogpt_medium\")\n","tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_dialogpt_medium\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","def generate_response(prompt, max_length=100):\n","    input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\").to(device)\n","    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n","    \n","    output = model.generate(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        max_length=max_length,\n","        num_return_sequences=1,\n","        no_repeat_ngram_size=3,\n","        do_sample=True,\n","        top_k=50,\n","        top_p=0.95,\n","        temperature=0.7,\n","        pad_token_id=tokenizer.eos_token_id,\n","        eos_token_id=tokenizer.eos_token_id,\n","    )\n","    \n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return response[len(prompt):].strip()\n","\n","# Update your prompts to be more specific\n","movie_test_prompts = [\n","    \"Question: Why does Cameron suggest that Bianca needs to learn how to lie?\",\n","    \"Statement: Describe how Bianca feels about becoming a persona she can't quit.\",\n","    \"Question: What specific comment did Guillermo make about Bianca's hair color?\",\n","    \"Statement: Explain Kat's bad experience with a guy who broke up with her.\",\n","    \"Question: How does Bianca react to Joey's aspirations in his modeling career?\",\n","]\n","\n","# Generate responses and evaluate\n","for prompt in movie_test_prompts:\n","    response = generate_response(prompt)\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated response: {response}\")\n","    print()\n","\n","# Calculate perplexity if needed\n","def calculate_perplexity(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n","    with torch.no_grad():\n","        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n","    return torch.exp(outputs.loss).item()\n","\n","# Calculate average perplexity\n","total_perplexity = 0\n","for prompt in movie_test_prompts:\n","    full_text = prompt + \" \" + generate_response(prompt)\n","    perplexity = calculate_perplexity(full_text)\n","    total_perplexity += perplexity\n","    print(f\"Perplexity for prompt: {perplexity:.2f}\")\n","\n","avg_perplexity = total_perplexity / len(movie_test_prompts)\n","print(f\"\\nAverage Perplexity: {avg_perplexity:.2f}\")"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T23:30:07.513793Z","iopub.status.busy":"2024-10-18T23:30:07.512906Z","iopub.status.idle":"2024-10-18T23:30:18.940177Z","shell.execute_reply":"2024-10-18T23:30:18.939231Z","shell.execute_reply.started":"2024-10-18T23:30:07.513753Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt: In the movie '10 Things I Hate About You', why does Cameron suggest that Bianca needs to learn how to lie?\n","Generated response: Because she was lying when she said she couldn't get married. She can't lie now. She was lying before she even told Bianca she was leaving. That's the whole point of the film. She's lying now because she's lying before Bianca even told her she's leaving. She lied before she could even say goodbye. That is the whole basis of the whole film. That means nothing to her, so she's not going to say goodbye to anyone. She lies now because it's the other side of the room. That proves that Biancas lying is still real and she's still there. It\n","Reference: Cameron suggests Bianca needs to learn how to lie because she's too honest and straightforward, which might not always work in her favor in social situations.\n","BLEU Score: 0.0610\n","Perplexity: 9.5903\n","\n","Prompt: In '10 Things I Hate About You', how does Kat feel about Bianca's attitude towards dating, considering their father's rule about Kat dating first?\n","Generated response: Well, I'd love to hear her say that to her face. I'm sure she'd love that. But I don't think it's something she'd say on TV. Bianca said she'd never dated before and Kat was like, oh she's good at it. She's got good feet. I want to do it. So do it and then we can talk about it. Do it and I'll never have to ask you again. Okay. Okay? Okay. I'll do it okay. Okay okay. I promise. I swear. Okay? Okay. OK.\n","Reference: Kat feels frustrated that Bianca's desire to date affects her own freedom and choices, as their father won't allow Bianca to date until Kat does.\n","BLEU Score: 0.5537\n","Perplexity: 11.2384\n","\n","Prompt: During a conversation about Bianca's appearance in '10 Things I Hate About You', what specific comment did the character Guillermo make about Bianca's hair color?\n","Generated response: He said that I had to grow it out because it was too dark in my hair at the time and that he didn't want to have it that way for Bianca because she was too beautiful to have dark hair in the first place. He was very supportive of Bianca though and I told him that it would be better for her if she went with the dark hair color. He said yes and I thought it was great. He's a very good man and I love him very much. He really cares about her very much and he's very passionate about her. I\n","Reference: Guillermo said that if Bianca's hair gets any lighter, she'll look like an extra on 90210.\n","BLEU Score: 0.1586\n","Perplexity: 8.6730\n","\n","Prompt: In a scene from '10 Things I Hate About You', how does Patrick react when Kat questions his motives for being with her?\n","Generated response: He doesn't know. He doesn't care. He's just happy. He wants to be with her. He just wants to. I'm so happy. I love you. I don't know what I would do without you. And you're the best thing that ever happened to me. You're so wonderful. I can't take it anymore. I just want to be here with you. With you and my baby. I want to know everything about you. You and that little girl. I know everything. I think I might die. I wish I had that much time. I really do want to\n","Reference: Patrick gets defensive and asks Kat if he needs to have a motive to be with her, implying that he genuinely likes her.\n","BLEU Score: 0.0246\n","Perplexity: 7.4348\n","\n","Prompt: In '10 Things I Hate About You', how does Joey respond when Kat confronts him and tells him to leave her sister Bianca alone?\n","Generated response: He laughs hysterically at the idea of her leaving Bianca and goes off to find her own mother.  Not that I'm complaining.  Joey's a lucky guy.  He's got a great life.  I'm lucky I have one.  You don't know what you're missing out on when you're out here in the world.  It's a beautiful world sometimes.  All you gotta do is find a good woman.  Then you can go on your own adventure.  And then you can have a nice big home.  So when you get married you can just sit\n","Reference: Joey dismisses Kat's request and asks why he would leave Bianca alone, showing his lack of respect for Kat's wishes.\n","BLEU Score: 0.5193\n","Perplexity: 9.3435\n","\n","Average BLEU Score: 0.2635\n","Average Perplexity: 9.2560\n"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.tokenize import word_tokenize\n","\n","# Load the fine-tuned model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_dialogpt_medium\")\n","tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_dialogpt_medium\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","def generate_response(prompt, max_length=150):\n","    input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\").to(device)\n","    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n","    \n","    output = model.generate(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        max_length=max_length,\n","        num_return_sequences=1,\n","        no_repeat_ngram_size=3,\n","        do_sample=True,\n","        top_k=50,\n","        top_p=0.95,\n","        temperature=0.7,\n","        pad_token_id=tokenizer.eos_token_id,\n","        eos_token_id=tokenizer.eos_token_id,\n","    )\n","    \n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return response[len(prompt):].strip()\n","\n","def calculate_bleu(reference, candidate):\n","    reference_tokens = word_tokenize(reference.lower())\n","    candidate_tokens = word_tokenize(candidate.lower())\n","    return sentence_bleu([reference_tokens], candidate_tokens)\n","\n","def calculate_perplexity(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n","    with torch.no_grad():\n","        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n","    return torch.exp(outputs.loss).item()\n","\n","# Updated test prompts with more context\n","movie_test_prompts = [\n","    \"In the movie '10 Things I Hate About You', why does Cameron suggest that Bianca needs to learn how to lie?\",\n","    \"In '10 Things I Hate About You', how does Kat feel about Bianca's attitude towards dating, considering their father's rule about Kat dating first?\",\n","    \"During a conversation about Bianca's appearance in '10 Things I Hate About You', what specific comment did the character Guillermo make about Bianca's hair color?\",\n","    \"In a scene from '10 Things I Hate About You', how does Patrick react when Kat questions his motives for being with her?\",\n","    \"In '10 Things I Hate About You', how does Joey respond when Kat confronts him and tells him to leave her sister Bianca alone?\"\n","]\n","\n","reference_answers = [\n","    \"Cameron suggests Bianca needs to learn how to lie because she's too honest and straightforward, which might not always work in her favor in social situations.\",\n","    \"Kat feels frustrated that Bianca's desire to date affects her own freedom and choices, as their father won't allow Bianca to date until Kat does.\",\n","    \"Guillermo said that if Bianca's hair gets any lighter, she'll look like an extra on 90210.\",\n","    \"Patrick gets defensive and asks Kat if he needs to have a motive to be with her, implying that he genuinely likes her.\",\n","    \"Joey dismisses Kat's request and asks why he would leave Bianca alone, showing his lack of respect for Kat's wishes.\"\n","]\n","\n","# Evaluate responses\n","total_bleu = 0\n","total_perplexity = 0\n","\n","for prompt, reference in zip(movie_test_prompts, reference_answers):\n","    generated_response = generate_response(prompt)\n","    bleu_score = calculate_bleu(reference, generated_response)\n","    perplexity = calculate_perplexity(prompt + \" \" + generated_response)\n","    \n","    total_bleu += bleu_score\n","    total_perplexity += perplexity\n","    \n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated response: {generated_response}\")\n","    print(f\"Reference: {reference}\")\n","    print(f\"BLEU Score: {bleu_score:.4f}\")\n","    print(f\"Perplexity: {perplexity:.4f}\")\n","    print()\n","\n","# Calculate average scores\n","avg_bleu = total_bleu / len(movie_test_prompts)\n","avg_perplexity = total_perplexity / len(movie_test_prompts)\n","print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n","print(f\"Average Perplexity: {avg_perplexity:.4f}\")"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T00:15:51.704895Z","iopub.status.busy":"2024-10-19T00:15:51.704271Z","iopub.status.idle":"2024-10-19T00:28:10.777647Z","shell.execute_reply":"2024-10-19T00:28:10.776298Z","shell.execute_reply.started":"2024-10-19T00:15:51.704857Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Chatbot: Hello! Let's talk about '10 Things I Hate About You'. What would you like to know?\n"]},{"name":"stdout","output_type":"stream","text":["You:  what is the capital of united states?\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: Washington D.C.  \"I hate all movies with a passion.\"  And you must be a very religious man. You must be very familiar with the phrase \"The more you preach the more you hate.\"  Now what is that supposed to\n"]},{"name":"stdout","output_type":"stream","text":["You:  You are correct. \n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: \"The more I preach the better I get.\"  I must have been preaching for years.  I'm not sure I even know what that phrase means.  Now, what is it?  Do you know what it means?  What do\n"]},{"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m             conversation_history \u001b[38;5;241m=\u001b[39m conversation_history[\u001b[38;5;241m-\u001b[39mMAX_HISTORY_TURNS \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[34], line 58\u001b[0m, in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: Hello! Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms talk about \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10 Things I Hate About You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. What would you like to know?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbye\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: Goodbye! It was nice chatting with you about \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10 Things I Hate About You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Load the model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_dialogpt_medium\")\n","tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_dialogpt_medium\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Constants\n","MAX_HISTORY_TURNS = 5\n","MAX_HISTORY_TOKENS = 512\n","\n","def generate_response(prompt, conversation_history, max_length=50):\n","    # Construct the full prompt with conversation history\n","    full_prompt = construct_prompt(conversation_history, prompt)\n","    \n","    input_ids = tokenizer.encode(full_prompt + tokenizer.eos_token, return_tensors=\"pt\").to(device)\n","    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n","    \n","    # Truncate if the input is too long\n","    if input_ids.shape[1] > MAX_HISTORY_TOKENS:\n","        input_ids = input_ids[:, -MAX_HISTORY_TOKENS:]\n","        attention_mask = attention_mask[:, -MAX_HISTORY_TOKENS:]\n","    \n","    output = model.generate(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        max_length=input_ids.shape[1] + max_length,\n","        num_return_sequences=1,\n","        no_repeat_ngram_size=3,\n","        do_sample=True,\n","        top_k=50,\n","        top_p=0.95,\n","        temperature=0.7,\n","        pad_token_id=tokenizer.eos_token_id,\n","        eos_token_id=tokenizer.eos_token_id,\n","    )\n","    \n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return response[len(full_prompt):].strip()\n","\n","def construct_prompt(conversation_history, current_prompt):\n","    prompt_parts = [\n","        \"The following is a conversation about movies, particularly '10 Things I Hate About You'. Respond in the style of the movie's characters:\",\n","        *conversation_history[-MAX_HISTORY_TURNS:],\n","        f\"Human: {current_prompt}\",\n","        \"AI:\"\n","    ]\n","    return \"\\n\".join(prompt_parts)\n","\n","def chat():\n","    conversation_history = []\n","    print(\"Chatbot: Hello! Let's talk about '10 Things I Hate About You'. What would you like to know?\")\n","    \n","    while True:\n","        user_input = input(\"You: \")\n","        if user_input.lower() in ['exit', 'quit', 'bye']:\n","            print(\"Chatbot: Goodbye! It was nice chatting with you about '10 Things I Hate About You'.\")\n","            break\n","        \n","        response = generate_response(user_input, conversation_history)\n","        print(f\"Chatbot: {response}\")\n","        \n","        # Update conversation history\n","        conversation_history.append(f\"Human: {user_input}\")\n","        conversation_history.append(f\"AI: {response}\")\n","        \n","        # Keep only the last MAX_HISTORY_TURNS turns\n","        if len(conversation_history) > MAX_HISTORY_TURNS * 2:\n","            conversation_history = conversation_history[-MAX_HISTORY_TURNS * 2:]\n","\n","if __name__ == \"__main__\":\n","    chat()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":18754,"sourceId":24465,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
